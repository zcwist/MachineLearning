<!DOCTYPE html><html><head><meta charset="utf-8"><style>html { font-size: 100%; overflow-y: scroll; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; }

body{
  color:#444;
  font-family:Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman',
              "Hiragino Sans GB", "STXihei", "微软雅黑", serif;
  font-size:12px;
  line-height:1.5em;
  background:#fefefe;
  width: 45em;
  margin: 10px auto;
  padding: 1em;
  outline: 1300px solid #FAFAFA;
}

a{ color: #0645ad; text-decoration:none;}
a:visited{ color: #0b0080; }
a:hover{ color: #06e; }
a:active{ color:#faa700; }
a:focus{ outline: thin dotted; }
a:hover, a:active{ outline: 0; }

span.backtick {
  border:1px solid #EAEAEA;
  border-radius:3px;
  background:#F8F8F8;
  padding:0 3px 0 3px;
}

::-moz-selection{background:rgba(255,255,0,0.3);color:#000}
::selection{background:rgba(255,255,0,0.3);color:#000}

a::-moz-selection{background:rgba(255,255,0,0.3);color:#0645ad}
a::selection{background:rgba(255,255,0,0.3);color:#0645ad}

p{
margin:1em 0;
}

img{
max-width:100%;
}

h1,h2,h3,h4,h5,h6{
font-weight:normal;
color:#111;
line-height:1em;
}
h4,h5,h6{ font-weight: bold; }
h1{ font-size:2.5em; }
h2{ font-size:2em; border-bottom:1px solid silver; padding-bottom: 5px; }
h3{ font-size:1.5em; }
h4{ font-size:1.2em; }
h5{ font-size:1em; }
h6{ font-size:0.9em; }

blockquote{
color:#666666;
margin:0;
padding-left: 3em;
border-left: 0.5em #EEE solid;
}
hr { display: block; height: 2px; border: 0; border-top: 1px solid #aaa;border-bottom: 1px solid #eee; margin: 1em 0; padding: 0; }


pre , code, kbd, samp { 
  color: #000; 
  font-family: monospace; 
  font-size: 0.88em; 
  border-radius:3px;
  background-color: #F8F8F8;
  border: 1px solid #CCC; 
}
pre { white-space: pre; white-space: pre-wrap; word-wrap: break-word; padding: 5px 12px;}
pre code { border: 0px !important; padding: 0;}
code { padding: 0 3px 0 3px; }

b, strong { font-weight: bold; }

dfn { font-style: italic; }

ins { background: #ff9; color: #000; text-decoration: none; }

mark { background: #ff0; color: #000; font-style: italic; font-weight: bold; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }
sup { top: -0.5em; }
sub { bottom: -0.25em; }

ul, ol { margin: 1em 0; padding: 0 0 0 2em; }
li p:last-child { margin:0 }
dd { margin: 0 0 0 2em; }

img { border: 0; -ms-interpolation-mode: bicubic; vertical-align: middle; }

table { border-collapse: collapse; border-spacing: 0; }
td { vertical-align: top; }

@media only screen and (min-width: 480px) {
body{font-size:14px;}
}

@media only screen and (min-width: 768px) {
body{font-size:16px;}
}

@media print {
  * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; }
  body{font-size:12pt; max-width:100%; outline:none;}
  a, a:visited { text-decoration: underline; }
  hr { height: 1px; border:0; border-bottom:1px solid black; }
  a[href]:after { content: " (" attr(href) ")"; }
  abbr[title]:after { content: " (" attr(title) ")"; }
  .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after { content: ""; }
  pre, blockquote { border: 1px solid #999; padding-right: 1em; page-break-inside: avoid; }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page :left { margin: 15mm 20mm 15mm 10mm; }
  @page :right { margin: 15mm 10mm 15mm 20mm; }
  p, h2, h3 { orphans: 3; widows: 3; }
  h2, h3 { page-break-after: avoid; }
}
</style><title>README</title></head><body><h1 id="plsi-implemention">PLSI Implemention</h1>
<p>机械系 张承巍 20133102106</p>
<h2 id="_1">模块说明</h2>
<h3 id="1mplsa">1.MPLSA</h3>
<p>用于PLSA算法的实现</p>
<p>主要方法有：</p>
<p>-<strong>init</strong>(docs, corpus, topics):构造函数</p>
<p>docs:文档集，采用List存储Dictionary，存储稀疏矩阵</p>
<p>corpus:语料库，存储所有的单词表，主要用于最后结果输出</p>
<p>topics:主题数</p>
<p>初始化过程中，随机生成规一化的P(z|d,w),进行第一步E步，然后进行M步。</p>
<p>-makeIndex():构造倒排索引</p>
<p>若遍历单词时，再遍历文本，时间成本很大，采用倒排索引，大大提高效率。</p>
<p>-eStep()：EM算法的E-STEP</p>
<p>-mStep(): EM算法的M-STEP</p>
<p>-train(): 训练。两次迭代Likelihood<strong>相对误差小于1e-4</strong>时，视为收敛。</p>
<h3 id="2preprocessingpy">2.PreProcessing.py</h3>
<p>对原文本进行预处理，产生文档集的稀疏矩阵，生成语料库</p>
<h3 id="3mainpy">3.main.py</h3>
<p>调用中间文间，运行PLSA算法，输出结果。</p>
<h2 id="_2">操作步骤</h2>
<h3 id="1">1.预处理</h3>
<pre><code>运行PreProcessing.py 进行前处理。可以通过改变topX，改变预处理的行数，大于总行数，则处理所以行。
</code></pre>
<h3 id="2">2.训练并输出</h3>
<pre><code>改变分类数topics，运行main.py 进行训练，并输出分类结果。
</code></pre>
<h2 id="_3">算法结果</h2>
<p>分别以分类数为2、3、5为例</p>
<h3 id="topics-2">topics = 2</h3>
<pre><code>spent 0.151679s on importing file
iter times = 22
likelihood = -454551.971408
-------------------------
topic 0:
web:0.024722
semantic:0.014825
data:0.012913
information:0.011012
support:0.010910
knowledge:0.009691
learning:0.008851
approach:0.008739
vector:0.007845
classification:0.007484
-------------------------
-------------------------
topic 1:
learning:0.033945
systems:0.011182
planning:0.010127
logic:0.007961
information:0.007730
reasoning:0.007477
model:0.007213
agents:0.007199
knowledge:0.007056
data:0.006972
-------------------------
spent 10.435883s on algorithm
[Finished in 10.7s]
</code></pre>
<p>分类解释：</p>
<p>-topic 0:语义网</p>
<p>-topic 1:机器学习</p>
<h3 id="topics-3">topics = 3</h3>
<pre><code>spent 0.140538s on importing file
iter times = 28
likelihood = -460331.539582
-------------------------
topic 0:
learning:0.031034
support:0.016292
systems:0.013215
classification:0.011959
vector:0.011954
networks:0.011668
data:0.009361
bayesian:0.009346
multi-agent:0.009153
approach:0.008402
-------------------------
-------------------------
topic 1:
learning:0.030496
data:0.013964
planning:0.012966
search:0.011529
information:0.011069
selection:0.010855
web:0.010801
feature:0.009650
decision:0.008272
based:0.007440
-------------------------
-------------------------
topic 2:
web:0.030849
knowledge:0.023412
semantic:0.020417
reasoning:0.014108
logic:0.011368
information:0.010255
ontology:0.009541
approach:0.008848
extraction:0.008796
programming:0.008621
-------------------------
spent 14.539366s on algorithm
[Finished in 14.7s]
</code></pre>
<p>分类解释：</p>
<p>-topic 0:学习系统</p>
<p>-topic 1:信息获取</p>
<p>-topic 2:语义网</p>
<h3 id="topics-5">topics = 5</h3>
<pre><code>spent 0.144273s on importing file
iter times = 32
likelihood = -467529.845755
-------------------------
topic 0:
web:0.042768
semantic:0.020797
ontology:0.016438
systems:0.016348
knowledge:0.015674
agent:0.014912
analysis:0.013815
ontologies:0.012599
applications:0.012427
model:0.011521
-------------------------
-------------------------
topic 1:
learning:0.020900
text:0.019279
agents:0.015186
networks:0.014300
planning:0.012251
classification:0.012191
intelligent:0.010968
environments:0.010441
mobile:0.010264
bayesian:0.009939
-------------------------
-------------------------
topic 2:
learning:0.036866
support:0.028806
logic:0.021087
vector:0.020710
language:0.020522
programming:0.016003
machine:0.015813
user:0.013175
modeling:0.012614
machines:0.012238
-------------------------
-------------------------
topic 3:
learning:0.029182
data:0.028981
web:0.026687
selection:0.018392
extraction:0.017149
information:0.016766
classification:0.014099
feature:0.014081
mining:0.013805
search:0.012353
-------------------------
-------------------------
topic 4:
planning:0.024653
learning:0.020392
approach:0.020011
systems:0.019629
knowledge:0.018474
multi-agent:0.015807
system:0.014063
data:0.012685
reasoning:0.012611
information:0.011110
-------------------------
spent 21.982122s on algorithm
[Finished in 22.2s]
</code></pre>
<p>随着分类数目的增多，对分类进行解释变得更加复杂，对这个领域不是太了解，就不能再做细分了。所以最大的分类数选择了5，在调试阶段也尝试过30类以上的分类操作。</p>
<h2 id="_4">特色</h2>
<p>-倒排索引</p>
<p>-list嵌套dictionary，存储稀疏矩阵</p>
<p>-尽可能用list，提高处理效率</p>
<h2 id="_5">一些体会</h2>
<p>以前以为空洞的概念论，现在发挥了大作用。不算特别复杂的公式推导，就能实现这样看起来复杂的功能。</p>
<p>算法的实现，倒不是很麻烦，但是优化就显得很难。</p>
<p>但是在一开始，仅选择100条记录作为sample，分5类，迭代9次，就花了6.8s，跑真实数据的10000多条数据的时候，我直接以为死机了。</p>
<p>后来增加了<strong>倒排索引</strong>的结构，在外层遍历词表的时候，内层不遍历所有文档，而只遍历包含该词的文档，这样一来，文档-词索引，词-文档索引来存储大型的稀疏矩阵，大大提高了效率。此时处理1000组数据，分5类，用时3.9s，但是在处理所有数据的时候，还是耗时433.16s。而这只是进行一种分类方式，这样的时间还是不可接受的。</p>
<p>下面我又进行了优化，可能是python内部的机制决定，对于dictionary的检索速度还是慢。我尽可能地使用list数据结构，又大幅度地减少了时间，对所有文档分5类，耗时22.2s。这样的速度就比较能接受了。</p>
<p>越是想缩短时间，就越是觉得自己基础知识的薄弱，但是不断优化的过程，优化的是程序，提高的是自己的能力。外系生做这个作业，难度是有的，但收获是更大的。</p>
<p>最后，感谢助教不厌其烦的耐心解答。</p>
<h2></h2></body></html>